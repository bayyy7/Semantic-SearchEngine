{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import signals\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import is_item, ItemAdapter\n",
    "\n",
    "\n",
    "class NarkotikaSpiderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, or item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Request or item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesnâ€™t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n",
    "\n",
    "\n",
    "class NarkotikaDownloaderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        # Called for each request that goes through the downloader\n",
    "        # middleware.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this request\n",
    "        # - or return a Response object\n",
    "        # - or return a Request object\n",
    "        # - or raise IgnoreRequest: process_exception() methods of\n",
    "        #   installed downloader middleware will be called\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info(\"Spider opened: %s\" % spider.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class NarkotikaPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class PidanaUmumSpider(scrapy.Spider):\n",
    "    name = \"scraper\"\n",
    "    allowed_domains = [\"putusan3.mahkamahagung.go.id\"]\n",
    "    start_urls = [\"https://putusan3.mahkamahagung.go.id/direktori/index/pengadilan/pn-jombang/kategori/narkotika-dan-psikotropika-1.html\"]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        next_page = response.xpath('//*[@id=\"tabs-1\"]/div[3]/nav/ul/li[5]/a[2]/@href').extract_first()\n",
    "        if next_page:\n",
    "            yield scrapy.Request(next_page, callback=self.parse_first)\n",
    "            \n",
    "    def parse_first(self, response):\n",
    "        per_link = response.css('div.spost.clearfix strong a')\n",
    "        for page in per_link:\n",
    "            link = page.attrib['href']\n",
    "            if link:\n",
    "                print(link)\n",
    "                yield scrapy.Request(link, callback=self.parse_second)\n",
    "    \n",
    "    def parse_second(self, response):\n",
    "        for selector in response.css('tbody'):\n",
    "            yield {\n",
    "                'Tanggal': selector.css('tr td h2::text')[1].get(),\n",
    "                'Penuntut Umum': selector.css('tr td h2::text')[2].get(),\n",
    "                'Terdakwa': selector.css('tr td h2::text')[4:].get(),\n",
    "                'Nomor Putusan': selector.css('tr td')[2].get(),\n",
    "                'Kata Kunci': selector.css('tr td')[8].get(),\n",
    "                'Tanggal Register': selector.css('tr td')[12].get(),\n",
    "                'Lembaga Peradilan': 'PN JOMBANG',\n",
    "                'Hakim Ketua': selector.css('tr td')[18].get(),\n",
    "                'Hakim Anggota': selector.css('tr td')[20].get(),\n",
    "                'Panitera': selector.css('tr td')[22].get(),\n",
    "                'Catatan Amar': selector.css('tr td')[28].get(),                \n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
